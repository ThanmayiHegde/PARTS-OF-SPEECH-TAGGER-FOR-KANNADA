{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a6eb320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af575cab",
   "metadata": {},
   "source": [
    "Reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08b09fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = codecs.open(\"corpus.txt\", \"r\", encoding=\"utf8\") \n",
    "\n",
    "raw = f.readlines()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2184b83",
   "metadata": {},
   "source": [
    "Lines is the list that appends list of sentences with each word in the form of a tuple with its corresponding POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82782671",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []  \n",
    "l = []\n",
    "\n",
    "for w in raw:\n",
    "    w = w.encode('utf-8')    #Splitting list of words and decoding from UTF-8\n",
    "    x = w.strip(b'\\n').split(b' ')\n",
    "    if x[0] == b'*' or x[0] == b'.' or len(x) < 2:\n",
    "        if len(l) > 0:\n",
    "            lines.append(l)\n",
    "        l = []\n",
    "        continue\n",
    "    l.append((x[0], x[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5b914",
   "metadata": {},
   "source": [
    "Sentences is a list of list of sentences split into words and Labels is a list of list of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e057cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "num_words = 0\n",
    "\n",
    "for l in lines:\n",
    "    s = []\n",
    "    lab = []\n",
    "    for w in l:\n",
    "        x = w[0].decode('utf-8')\n",
    "        y = w[1].decode('utf-8')\n",
    "        s.append(x)\n",
    "        num_words += 1\n",
    "        lab.append(y)\n",
    "    labels.append(lab)\n",
    "    sentences.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f520b",
   "metadata": {},
   "source": [
    "Create a Word2vec model and build vocabulary from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b74a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec   \n",
    "min_count = 1\n",
    "window = 5\n",
    "\n",
    "model = Word2Vec(window=window, min_count=min_count)\n",
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639258a",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9c07e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "Iteration  100\n",
      "Iteration  200\n",
      "Iteration  300\n",
      "Iteration  400\n",
      "Iteration  500\n",
      "Iteration  600\n",
      "Iteration  700\n",
      "Iteration  800\n",
      "Iteration  900\n"
     ]
    }
   ],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings(action='ignore',category=Warning,module='gensim')   \n",
    "for i in range(1000):\n",
    "    model.train(sentences, total_words=num_words, epochs=1)    \n",
    "    if i%100 == 0:\n",
    "        print(\"Iteration \",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989c3e1",
   "metadata": {},
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f79356",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for idx,s in enumerate(sentences):          \n",
    "    x = []\n",
    "    yy = []\n",
    "    for wi,w in enumerate(s):\n",
    "        if w in model.wv:\n",
    "            m = model.wv[w]\n",
    "            if wi > 1:\n",
    "                m = np.hstack([m,model.wv[s[wi-1]]])\n",
    "            else:\n",
    "                m = np.hstack([m,np.zeros_like(model.wv[w])])\n",
    "            \n",
    "            if wi > 2:\n",
    "                m = np.hstack([m,model.wv[s[wi-2]]])\n",
    "            else:\n",
    "                m = np.hstack([m,np.zeros_like(model.wv[w])])\n",
    "            \n",
    "            if wi > 3:\n",
    "              m = np.hstack([m,model.wv[s[wi-3]]])\n",
    "            else:\n",
    "                m = np.hstack([m,np.zeros_like(model.wv[w])])\n",
    "            \n",
    "            if wi < len(s)-1:\n",
    "                m = np.hstack([m,model.wv[s[wi+1]]])\n",
    "            else:\n",
    "                m = np.hstack([m,np.zeros_like(model.wv[w])])\n",
    "            \n",
    "            if wi < len(s)-2:\n",
    "                m = np.hstack([m,model.wv[s[wi+2]]])\n",
    "            else:\n",
    "                m = np.hstack([m,np.zeros_like(model.wv[w])])\n",
    "            \n",
    "            if wi < len(s)-3:\n",
    "                m = np.hstack([m,model.wv[s[wi+3]]])\n",
    "            else:\n",
    "                m = np.hstack([m,np.zeros_like(model.wv[w])])\n",
    "            x.append(m)\n",
    "    offset = 100 - len(x)\n",
    "    pad_x = np.zeros_like(model.wv[w]) - 1\n",
    "    pad_x = np.hstack([pad_x,pad_x,pad_x,pad_x,pad_x, pad_x, pad_x])\n",
    "    yy = yy + labels[idx]\n",
    "    for i in range(offset):\n",
    "        x.append(pad_x)\n",
    "        yy.append(\"IRR\")\n",
    "    X.append(x)\n",
    "    y.append(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540d51f",
   "metadata": {},
   "source": [
    "X contains an array of vector embeddings and y is an array of POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c5375f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CC', 'DEM', 'DET', 'INJ', 'IRR', 'JJ', 'NN', 'NUM', 'PRP', 'PSP',\n",
       "       'QC', 'RB', 'SYM', 'UT', 'VM', 'WQ'], dtype='<U3')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd1ceae",
   "metadata": {},
   "source": [
    "Encoding target variables(POS tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb15d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_shape = y.shape\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()               \n",
    "y = le.fit_transform(y.ravel()).reshape(original_shape)\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0e701",
   "metadata": {},
   "source": [
    "Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db72170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"kannada-features.numpy\", \"wb\")\n",
    "np.save(f, X)                             \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b502f",
   "metadata": {},
   "source": [
    "Saving labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "029dd3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"kannada-labels.numpy\", \"wb\")\n",
    "np.save(f,y)                              \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1d01d",
   "metadata": {},
   "source": [
    "Implementing the Conditional Random Field(CRF) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aa48aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"kannada-features.numpy\").astype(np.float32)\n",
    "Y = np.load(\"kannada-labels.numpy\").astype(np.int32)\n",
    "\n",
    "np.random.seed(546)\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f51da0",
   "metadata": {},
   "source": [
    "Obtaining the count of occurences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ba0b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples, num_words, num_features = X.shape\n",
    "num_tags = np.unique(Y).size\n",
    "\n",
    "sequence_lengths = np.full(num_examples, 0, dtype=np.int32)\n",
    "for idx, row in enumerate(X):\n",
    "    count = 0\n",
    "    for word in row:\n",
    "        #print(word)\n",
    "        if np.all(word == -1):\n",
    "            break\n",
    "        count += 1\n",
    "    sequence_lengths[idx] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05db477",
   "metadata": {},
   "source": [
    "Splitting the corpus for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe5a172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 100\n",
    "x_test = X[-split:,:,:]\n",
    "y_test = Y[-split:,:]\n",
    "s_test = sequence_lengths[-split:]\n",
    "\n",
    "x = X[0:-split,:,:]\n",
    "y = Y[0:-split,:]\n",
    "sequence_lengths = sequence_lengths[0:-split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cdcb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb90be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in c:\\users\\vindh\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vindh\\anaconda3\\lib\\site-packages (from tensorflow_addons) (21.3)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\vindh\\anaconda3\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vindh\\anaconda3\\lib\\site-packages (from packaging->tensorflow_addons) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb85fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1734816",
   "metadata": {},
   "source": [
    "Building the model and printing the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ff71004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training set):  6.248801993482845\n",
      "Classification Accuracy (Training set):  51.75388154111558\n",
      "Classification Accuracy (Training set):  61.74046386812344\n",
      "Classification Accuracy (Training set):  65.89994249568717\n",
      "Classification Accuracy (Training set):  68.50680467701744\n",
      "Classification Accuracy (Training set):  70.02108491470193\n",
      "Classification Accuracy (Training set):  71.32451600536707\n",
      "Classification Accuracy (Training set):  72.16791259344451\n",
      "Classification Accuracy (Training set):  72.68545140885567\n",
      "Classification Accuracy (Training set):  73.35633505846272\n",
      "Classification Accuracy (Training set):  73.96971439524631\n",
      "Classification Accuracy (Training set):  74.85144719187272\n",
      "Classification Accuracy (Training set):  75.21564117308799\n",
      "Classification Accuracy (Training set):  75.4839946329308\n",
      "Classification Accuracy (Training set):  75.90569292696952\n",
      "Classification Accuracy (Training set):  76.38489553383171\n",
      "Classification Accuracy (Training set):  76.55740847230209\n",
      "Classification Accuracy (Training set):  76.69158520222351\n",
      "Classification Accuracy (Training set):  76.88326624496837\n",
      "Classification Accuracy (Training set):  77.03661107916427\n",
      "-------------------------------------------------\n",
      "Classification Accuracy (Test set):  66.85714285714286\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.compat.v1.Session() as session:\n",
    "        x_t = tf.constant(x)\n",
    "        xt_t = tf.constant(x_test)\n",
    "        y_t = tf.constant(y)\n",
    "        yt_t = tf.constant(y_test)\n",
    "        sequence_lengths_t = tf.constant(sequence_lengths)\n",
    "        st_t = tf.constant(s_test)\n",
    "        \n",
    "        weights = tf.compat.v1.get_variable(\"weights\", [num_features, num_tags])\n",
    "        matricized_x_t = tf.reshape(x_t, [-1, num_features])\n",
    "        matricized_unary_scores = tf.matmul(matricized_x_t, weights)\n",
    "        unary_scores = tf.reshape(matricized_unary_scores, [num_examples-split, num_words, num_tags])\n",
    "        \n",
    "        matricized_xt_t = tf.reshape(xt_t, [-1, num_features])\n",
    "        matricized_ust = tf.matmul(matricized_xt_t, weights)\n",
    "        ust = tf.reshape(matricized_ust, [split, num_words, num_tags])\n",
    "        \n",
    "        log_likelihood, transition_params = tfa.text.crf_log_likelihood(unary_scores, y_t, sequence_lengths_t)\n",
    "        \n",
    "        loss = tf.reduce_mean(-log_likelihood)\n",
    "        train_op = tf.train.GradientDescentOptimizer(0.03).minimize(loss)\n",
    "        \n",
    "        session.run(tf.global_variables_initializer())\n",
    "        for i in range(100):\n",
    "            tf_ust, tf_unary_scores, tf_transition_params, _ = session.run([ust, unary_scores, transition_params, train_op])\n",
    "            if i%5 == 0:\n",
    "                correct_labels = 0\n",
    "                total_labels = 0\n",
    "                for tf_unary_scores_, y_, sequence_length_ in zip(tf_unary_scores, y, sequence_lengths):\n",
    "                    tf_unary_scores_ = tf_unary_scores_[:sequence_length_]\n",
    "                    y_ = y_[:sequence_length_]\n",
    "                    \n",
    "                    viterbi_sequence, _ = tfa.text.viterbi_decode(tf_unary_scores_, tf_transition_params)\n",
    "                    \n",
    "                    correct_labels += np.sum(np.equal(viterbi_sequence, y_))\n",
    "                    total_labels += sequence_length_\n",
    "                accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "                print(\"Classification Accuracy (Training set): \", accuracy)\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "        pred_labels = []\n",
    "        actual_labels = []\n",
    "        for a, b, c in zip(tf_ust, y_test, s_test):\n",
    "            a = a[:c]\n",
    "            b = b[:c]\n",
    "            \n",
    "            vs, _ = tfa.text.viterbi_decode(a, tf_transition_params)\n",
    "            correct_labels += np.sum(np.equal(vs, b))\n",
    "            total_labels += c\n",
    "            \n",
    "            actual_labels = actual_labels + b.tolist()\n",
    "            pred_labels = pred_labels + vs\n",
    "            \n",
    "        accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(\"Classification Accuracy (Test set): \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ed6c9",
   "metadata": {},
   "source": [
    "Displaying precision, recall, f1-score and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4e95860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.80      0.53      0.64        15\n",
      "         DEM       0.83      0.59      0.69        17\n",
      "         DET       0.00      0.00      0.00        12\n",
      "          JJ       0.20      0.10      0.14        49\n",
      "          NN       0.63      0.77      0.69       306\n",
      "         NUM       0.00      0.00      0.00         2\n",
      "         PRP       0.74      0.63      0.68       110\n",
      "         PSP       0.75      0.21      0.33        14\n",
      "          QC       0.67      0.50      0.57        20\n",
      "          RB       0.54      0.30      0.39        46\n",
      "         SYM       0.94      0.99      0.96        83\n",
      "          UT       1.00      0.89      0.94         9\n",
      "          VM       0.65      0.74      0.69       190\n",
      "          WQ       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.67       875\n",
      "   macro avg       0.55      0.45      0.48       875\n",
      "weighted avg       0.65      0.67      0.65       875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = np.array(['CC', 'DEM', 'DET', 'INJ', 'IRR', 'JJ', 'NN', 'NUM', 'PRP', 'PSP',\n",
    "       'QC', 'RB', 'SYM', 'UT', 'VM', 'WQ'])\n",
    "\n",
    "print(classification_report(actual_labels, pred_labels, target_names=target_names[np.unique(actual_labels)].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c062c",
   "metadata": {},
   "source": [
    "Comparison between the Predicated labels and the actual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bf35896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('“', 'SYM'), ('ನನ್ನ', 'PRP'), ('ತಂಟೆಗೆ', 'NN'), ('ಬರಬೇಡ', 'VM'), ('ನಾನೀಗ', 'PRP'), ('ಧರ್ಮದತ್ತನ', 'NN'), ('ಮನೆಗೆ', 'NN'), ('ಹೋಗುತ್ತಿದ್ದೇನೆ', 'VM'), ('ಇದೇ', 'PRP'), ('ದಾರಿಯಲ್ಲಿ', 'NN'), ('ಹಿಂದಕ್ಕೆ', 'RB'), ('ಬರುವೆ', 'VM'), ('ಅದುವರೆಗೆ', 'PSP'), ('ಸುಮ್ಮನಿರು', 'VM'), ('”', 'SYM'), ('ಎಂದಳು', 'VM'), ('ಅವಳ', 'PRP'), ('ಮಾತಿಗೆ', 'NN'), ('ಕಳ್ಳ', 'NN'), ('ಗಹ', 'RB'), ('ಗಹಿಸಿ', 'DET'), ('ನಕ್ಕ', 'VM'), ('“', 'SYM'), ('ಅವಳೆಲ್ಲಿ', 'PRP'), ('ಬರುವಳು', 'VM'), ('?', 'SYM'), ('“', 'SYM'), ('ಎಂದುಕೊಂಡ', 'VM'), ('ಆದರೂ', 'RB'), ('ಅವಳನ್ನು', 'PRP'), ('ತಡೆಯದೆ', 'VM'), (',', 'SYM'), ('ಹೋಗಲು', 'VM'), ('ಬಿಟ್ಟ', 'VM'), ('ಧರ್ಮದತ್ತನ', 'NN'), ('ಮನೆಗೆ', 'NN'), ('ಅವಳು', 'PRP'), ('ಹೋದಾಗ', 'VM'), ('ಅವನಿಗೆ', 'PRP'), ('ತಬ್ಬಿಬ್ಬಾಯಿತು', 'VM'), ('“', 'SYM'), ('ಮಾತುಕೊಟ್ಟು', 'VM'), ('ಹೀಗೆ', 'RB'), ('ಬಂದವರುಂಟೆ', 'VM'), ('?', 'SYM'), ('“', 'SYM'), ('ಎಂದುಕ಼ೊಂಡ', 'VM'), ('“', 'SYM'), ('ಇವಳು', 'PRP'), ('ನಡತೆ', 'NN'), ('ಕೆಟ್ಟವಳಲ್ಲ', 'VM'), ('ನನ್ನ', 'PRP'), ('ಸ್ನೇಹಿತನ', 'NN'), ('ತಂಗಿ', 'NN'), (',', 'SYM'), ('ಮದುವೆಯಾದವಳು', 'VM'), ('”', 'SYM'), ('ಎಂದು', 'UT'), ('ಯೋಚಿಸಿ', 'VM'), ('ಅವಳನ್ನು', 'PRP'), ('ಮರ್ಯಾದೆಯಿಂದ', 'JJ'), ('ಹಿಂದಕ್ಕೆ', 'RB'), ('ಕಳುಹಿಸಿಕೊಟ್ಟ', 'VM'), ('ಅವಳು', 'PRP'), ('ಧರ್ಮದತ್ತನಿಂದ', 'NN'), ('ಸುರಕ್ಷಿತವಾಗಿ', 'JJ'), ('ಪಾರಾಗಿ', 'VM'), ('ಬೇಗ', 'RB'), ('ಬಂದುದಕ್ಕೆ', 'VM'), ('ಅವನು', 'PRP'), ('ಸಂತೋಷಪಟ್ಟ', 'VM'), ('“', 'SYM'), ('ನಿನ಼್ನ', 'RB'), ('ಹಣವೂ', 'NN'), ('ಬೇಡ', 'VM'), (',', 'SYM'), ('ನೀನೂ', 'PRP'), ('ಬೇಡ', 'VM'), (',', 'SYM'), ('ಬೇಗ', 'RB'), ('ಮನೆಗೆ', 'NN'), ('ಹೋಗಿ', 'VM'), ('ಸೇರಿಕೋ', 'VM'), ('”', 'SYM'), ('ಎಂದ', 'VM'), ('ಮನೆಗೆ', 'NN'), ('ಬಂದ', 'VM'), ('ಮೇಲೆ', 'RB'), ('ಅವಳ', 'PRP'), ('ಗಂಡ', 'NN'), ('ಸುದ್ದಿಯನ್ನೆಲ್ಲ', 'NN'), ('ಕೇಳಿ', 'VM'), ('ತಿಳಿದ', 'VM'), ('ಅವನಿಗೆ', 'PRP'), ('ಧರ್ಮದತ್ತನ', 'NN'), ('ಮೇಲೆಯೂ', 'RB'), ('ಕಳ್ಳನ', 'NN'), ('ಮೇಲೆಯೂ', 'RB'), ('ಆದರ', 'NN'), ('ಮೂಡಿತು', 'VM')]\n"
     ]
    }
   ],
   "source": [
    "lines = []  \n",
    "l = []\n",
    "\n",
    "for w in raw:\n",
    "    w = w.encode('utf-8')    #Splitting list of words and decoding from UTF-8\n",
    "    x = w.strip(b'\\n').split(b' ')\n",
    "    if x[0] == b'*' or x[0] == b'.' or len(x) < 2:\n",
    "        if len(l) > 0:\n",
    "            lines.append(l)\n",
    "        l = []\n",
    "        continue\n",
    "    l.append((x[0], x[1]))\n",
    "    \n",
    "x_test = sentences[-100:]\n",
    "y_test = labels[-100:]\n",
    "l1=[]                                      # Actual\n",
    "for i in range(len(x_test)):\n",
    "    l1.extend(list(zip(x_test[i],y_test[i])))\n",
    "print(l1[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dfe5b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('“', 'NN'), ('ನನ್ನ', 'NN'), ('ತಂಟೆಗೆ', 'NN'), ('ಬರಬೇಡ', 'NN'), ('ನಾನೀಗ', 'NN'), ('ಧರ್ಮದತ್ತನ', 'NN'), ('ಮನೆಗೆ', 'SYM'), ('ಹೋಗುತ್ತಿದ್ದೇನೆ', 'NN'), ('ಇದೇ', 'NN'), ('ದಾರಿಯಲ್ಲಿ', 'NN'), ('ಹಿಂದಕ್ಕೆ', 'SYM'), ('ಬರುವೆ', 'NN'), ('ಅದುವರೆಗೆ', 'NN'), ('ಸುಮ್ಮನಿರು', 'NN'), ('”', 'SYM'), ('ಎಂದಳು', 'NN'), ('ಅವಳ', 'NN'), ('ಮಾತಿಗೆ', 'NN'), ('ಕಳ್ಳ', 'SYM'), ('ಗಹ', 'NN'), ('ಗಹಿಸಿ', 'NN'), ('ನಕ್ಕ', 'NN'), ('“', 'NN'), ('ಅವಳೆಲ್ಲಿ', 'NN'), ('ಬರುವಳು', 'SYM'), ('?', 'NN'), ('“', 'NN'), ('ಎಂದುಕೊಂಡ', 'NN'), ('ಆದರೂ', 'NN'), ('ಅವಳನ್ನು', 'NN'), ('ತಡೆಯದೆ', 'SYM'), (',', 'NN'), ('ಹೋಗಲು', 'NN'), ('ಬಿಟ್ಟ', 'NN'), ('ಧರ್ಮದತ್ತನ', 'NN'), ('ಮನೆಗೆ', 'NN'), ('ಅವಳು', 'SYM'), ('ಹೋದಾಗ', 'NN'), ('ಅವನಿಗೆ', 'NN'), ('ತಬ್ಬಿಬ್ಬಾಯಿತು', 'NN'), ('“', 'NN'), ('ಮಾತುಕೊಟ್ಟು', 'NN'), ('ಹೀಗೆ', 'SYM'), ('ಬಂದವರುಂಟೆ', 'NN'), ('?', 'NN'), ('“', 'NN'), ('ಎಂದುಕ಼ೊಂಡ', 'NN'), ('“', 'NN'), ('ಇವಳು', 'NN'), ('ನಡತೆ', 'SYM'), ('ಕೆಟ್ಟವಳಲ್ಲ', 'NN'), ('ನನ್ನ', 'NN'), ('ಸ್ನೇಹಿತನ', 'NN'), ('ತಂಗಿ', 'SYM'), (',', 'NN'), ('ಮದುವೆಯಾದವಳು', 'NN'), ('”', 'NN'), ('ಎಂದು', 'NN'), ('ಯೋಚಿಸಿ', 'VM'), ('ಅವಳನ್ನು', 'VM'), ('ಮರ್ಯಾದೆಯಿಂದ', 'SYM'), ('ಹಿಂದಕ್ಕೆ', 'PRP'), ('ಕಳುಹಿಸಿಕೊಟ್ಟ', 'NN'), ('ಅವಳು', 'NN'), ('ಧರ್ಮದತ್ತನಿಂದ', 'NN'), ('ಸುರಕ್ಷಿತವಾಗಿ', 'SYM'), ('ಪಾರಾಗಿ', 'NN'), ('ಬೇಗ', 'NN'), ('ಬಂದುದಕ್ಕೆ', 'NN'), ('ಅವನು', 'NN'), ('ಸಂತೋಷಪಟ್ಟ', 'VM'), ('“', 'NN'), ('ನಿನ಼್ನ', 'NN'), ('ಹಣವೂ', 'SYM'), ('ಬೇಡ', 'NN'), (',', 'NN'), ('ನೀನೂ', 'NN'), ('ಬೇಡ', 'NN'), (',', 'VM'), ('ಬೇಗ', 'VM'), ('ಮನೆಗೆ', 'SYM'), ('ಹೋಗಿ', 'PRP'), ('ಸೇರಿಕೋ', 'NN'), ('”', 'VM'), ('ಎಂದ', 'JJ'), ('ಮನೆಗೆ', 'NN'), ('ಬಂದ', 'NN'), ('ಮೇಲೆ', 'SYM'), ('ಅವಳ', 'NN'), ('ಗಂಡ', 'NN'), ('ಸುದ್ದಿಯನ್ನೆಲ್ಲ', 'NN'), ('ಕೇಳಿ', 'NN'), ('ತಿಳಿದ', 'VM'), ('ಅವನಿಗೆ', 'NN'), ('ಧರ್ಮದತ್ತನ', 'NN'), ('ಮೇಲೆಯೂ', 'SYM'), ('ಕಳ್ಳನ', 'NN'), ('ಮೇಲೆಯೂ', 'NN'), ('ಆದರ', 'NN'), ('ಮೂಡಿತು', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = le.inverse_transform(pred_labels[-100:])           # Predicted\n",
    "l=[]\n",
    "for i in range(len(x_test)):\n",
    "    l.extend(list(zip(x_test[i],pred)))\n",
    "print(l[-100:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
